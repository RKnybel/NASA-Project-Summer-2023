{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495cb717",
   "metadata": {},
   "source": [
    "## Part 1/3: Prepare Hardware and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow and other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import os\n",
    "img_folder = \"msl-images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow GPU memory allocation fix\n",
    "#https://github.com/tensorflow/tensorflow/issues/35264\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452ffa1",
   "metadata": {},
   "source": [
    "## Input pipeline\n",
    "Using the Keras ImageDataGenerator, the dataset is split into train and validation in an 80/20 split. Class weights are calulated after loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42443936/keras-split-train-test-set-when-using-imagedatagenerator\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set the image size and batch size\n",
    "img_height = 227\n",
    "img_width = 227\n",
    "batch_size = 32\n",
    "\n",
    "# Create ImageDataGenerator for data augmentation and loading the dataset\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255, # Normalize pixel values to [0, 1]\n",
    "    validation_split=0.2 # set validation split\n",
    "    # Add other data augmentation parameters as needed\n",
    ")\n",
    "\n",
    "# Load the dataset using ImageDataGenerator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    img_folder,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,       # Important: Shuffle the data to avoid issues with class weighting\n",
    "    seed=123,            # Set seed for reproducibility\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    img_folder,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,       # Important: Shuffle the data to avoid issues with class weighting\n",
    "    seed=123,            # Set seed for reproducibility\n",
    "    subset='validation')\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_generator.classes), y=train_generator.classes)\n",
    "\n",
    "\n",
    "# Convert class weights to a dictionary format\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(\"\")\n",
    "print(class_weights_dict)\n",
    "\n",
    "#show some images\n",
    "fig, ax = plt.subplots(nrows=5, ncols=10, figsize=(15,15))\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(10):\n",
    "\n",
    "        # convert to unsigned integers for plotting\n",
    "        image = next(train_generator)[0][0]\n",
    "\n",
    "        # changing size from (1, 200, 200, 3) to (200, 200, 3) for plotting the image\n",
    "        image = np.squeeze(image)\n",
    "\n",
    "        # plot raw pixel data\n",
    "        ax[i][j].imshow(image)\n",
    "        ax[i][j].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9e0a3",
   "metadata": {},
   "source": [
    "## Here, the dataset's class distribution is shown to verify the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the number of classes\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# Count the number of samples for each class in the training data\n",
    "train_class_counts = np.zeros(num_classes)\n",
    "for _, labels in train_generator:\n",
    "    for label in labels.argmax(axis=1):\n",
    "        train_class_counts[label] += 1\n",
    "    if train_generator.batch_index == 0:\n",
    "        break\n",
    "\n",
    "# Count the number of samples for each class in the validation data\n",
    "validation_class_counts = np.zeros(num_classes)\n",
    "for _, labels in validation_generator:\n",
    "    for label in labels.argmax(axis=1):\n",
    "        validation_class_counts[label] += 1\n",
    "    if validation_generator.batch_index == 0:\n",
    "        break\n",
    "\n",
    "# Get the class labels\n",
    "class_labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "# Plot the class distribution for training data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(class_labels, train_class_counts)\n",
    "plt.title(\"Class Distribution - Training Data\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add number labels to each bar\n",
    "for i, count in enumerate(train_class_counts):\n",
    "    plt.text(i, count, str(int(count)), ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot the class distribution for validation data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(class_labels, validation_class_counts)\n",
    "plt.title(\"Class Distribution - Validation Data\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add number labels to each bar\n",
    "for i, count in enumerate(validation_class_counts):\n",
    "    plt.text(i, count, str(int(count)), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9d168",
   "metadata": {},
   "source": [
    "## Part 2/3: Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7935e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#alexnet\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(img_height, img_width,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554026ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision and recall metrics\n",
    "#These were removed from Keras because they may be inaccurate due to being computed batchwise\n",
    "# https://stackoverflow.com/questions/73564461/recall-and-precision-metrics-for-multi-class-classification-in-tensorflow-keras\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy', precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e4c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the layers!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63306ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training parameters\n",
    "from keras import backend as K\n",
    "K.set_value(model.optimizer.learning_rate, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd074d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model, time to cook!\n",
    "\n",
    "\n",
    "#set up tensorboard\n",
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "epochs=20\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = validation_generator.samples // batch_size,\n",
    "    callbacks = [tensorboard_callback]\n",
    "    #class_weight = class_weights_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98553c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ad273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize training results\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a0cb9",
   "metadata": {},
   "source": [
    "## Part 3/3: Evaluation and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68375a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the accuracy\n",
    "score = model.evaluate(validation_generator, verbose=0)\n",
    "print(\"Validation Loss: \" + str(score[0]))\n",
    "print(\"Validation Accuracy: \" + str(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab18682c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
