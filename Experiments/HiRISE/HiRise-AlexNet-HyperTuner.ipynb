{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495cb717",
   "metadata": {},
   "source": [
    "## Part 1/3: Prepare Hardware and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea1ac39",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2545953505.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_17012/2545953505.py\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    import keras_tuner as ktFUCK YOU\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow and other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "import os\n",
    "img_folder = \"map-proj-v3-classdirs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow GPU memory allocation fix\n",
    "#https://github.com/tensorflow/tensorflow/issues/35264\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452ffa1",
   "metadata": {},
   "source": [
    "## Input pipeline\n",
    "Using the Keras ImageDataGenerator, the dataset is split into train and validation in an 80/20 split. Class weights are calulated after loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42443936/keras-split-train-test-set-when-using-imagedatagenerator\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set the image size and batch size\n",
    "img_height = 227\n",
    "img_width = 227\n",
    "batch_size = 128\n",
    "\n",
    "# Create ImageDataGenerator for data augmentation and loading the dataset\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255, # Normalize pixel values to [0, 1]\n",
    "    validation_split=0.2 # set validation split\n",
    "    # Add other data augmentation parameters as needed\n",
    ")\n",
    "\n",
    "# Load the dataset using ImageDataGenerator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    img_folder,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,       # Important: Shuffle the data to avoid issues with class weighting\n",
    "    seed=123,            # Set seed for reproducibility\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    img_folder,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,       # Important: Shuffle the data to avoid issues with class weighting\n",
    "    seed=123,            # Set seed for reproducibility\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9e0a3",
   "metadata": {},
   "source": [
    "## Here, the dataset's class distribution is shown to verify the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1f6f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the number of classes\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# Count the number of samples for each class in the training data\n",
    "train_class_counts = np.zeros(num_classes)\n",
    "for _, labels in train_generator:\n",
    "    for label in labels.argmax(axis=1):\n",
    "        train_class_counts[label] += 1\n",
    "    if train_generator.batch_index == 0:\n",
    "        break\n",
    "\n",
    "# Count the number of samples for each class in the validation data\n",
    "validation_class_counts = np.zeros(num_classes)\n",
    "for _, labels in validation_generator:\n",
    "    for label in labels.argmax(axis=1):\n",
    "        validation_class_counts[label] += 1\n",
    "    if validation_generator.batch_index == 0:\n",
    "        break\n",
    "\n",
    "# Get the class labels\n",
    "class_labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "# Plot the class distribution for training data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(class_labels, train_class_counts)\n",
    "plt.title(\"Class Distribution - Training Data\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add number labels to each bar\n",
    "for i, count in enumerate(train_class_counts):\n",
    "    plt.text(i, count, str(int(count)), ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot the class distribution for validation data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(class_labels, validation_class_counts)\n",
    "plt.title(\"Class Distribution - Validation Data\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add number labels to each bar\n",
    "for i, count in enumerate(validation_class_counts):\n",
    "    plt.text(i, count, str(int(count)), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9d168",
   "metadata": {},
   "source": [
    "## Part 2/3: Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypermodel\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "def model_builder(hp):\n",
    "    # 5 convolutional layers\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(filters=96, input_shape=(227,227,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Max Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Max Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Max Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "    # Passing it to a Fully Connected layer\n",
    "    model.add(Flatten())\n",
    "    # 1st Fully Connected Layer\n",
    "    model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout to prevent overfitting\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # 2nd Fully Connected Layer\n",
    "    model.add(Dense(4096))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # 3rd Fully Connected Layer\n",
    "    model.add(Dense(1000))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(7))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # learning rates: 0.01, 0.001, 0.0001, 0.00001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63306ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tuner\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=20,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='HiRiseV3Net')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd074d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(training_data=train_generator, \n",
    "             validation_data=validation_generator, \n",
    "             epochs=50, \n",
    "            steps_per_epoch = train_generator.samples // batch_size,\n",
    "        validation_steps = validation_generator.samples // batch_size,\n",
    "             callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98553c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ad273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize training results\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt_size = 10\n",
    "\n",
    "plt.figure(figsize=(plt_size, plt_size/2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a0cb9",
   "metadata": {},
   "source": [
    "## Part 3/3: Evaluation and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68375a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the accuracy\n",
    "score = model.evaluate(validation_generator, verbose=0)\n",
    "print(\"Validation Loss: \" + str(score[0]))\n",
    "print(\"Validation Accuracy: \" + str(score[1]))\n",
    "\n",
    "print(\"Min Validation Loss: \" + str(min(val_loss)))\n",
    "print(\"Max Validation Accuracy: \" + str(max(val_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab18682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the confusion matrix\n",
    "\n",
    "# predict on the validation dataset\n",
    "validation_predictions = model.predict(validation_generator)\n",
    "# Convert predicted probabilities to class labels\n",
    "predicted_labels = np.argmax(validation_predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(validation_generator.classes, predicted_labels)\n",
    "rel_confusion = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b251709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(rel_confusion, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446bd115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "# Calculate precision and recall for each class\n",
    "precision_per_class = precision_score(validation_generator.classes, predicted_labels, average=None)\n",
    "recall_per_class = recall_score(validation_generator.classes, predicted_labels, average=None)\n",
    "\n",
    "# Display precision and recall for each class\n",
    "for i, class_label in enumerate(class_labels):\n",
    "    print(f\"Class: {class_label}\")\n",
    "    print(f\"Precision: {precision_per_class[i]:.2f}\")\n",
    "    print(f\"Recall: {recall_per_class[i]:.2f}\")\n",
    "    print(\"------------\")\n",
    "\n",
    "# Calculate and display overall precision and recall\n",
    "overall_precision = precision_score(validation_generator.classes, predicted_labels, average='weighted')\n",
    "overall_recall = recall_score(validation_generator.classes, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"Overall Precision:\", overall_precision)\n",
    "print(\"Overall Recall:\", overall_recall)\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep = classification_report(validation_generator.classes, predicted_labels, target_names=class_labels)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
